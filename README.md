# Web3 Crack

üëâ [http://web3.cas-ll.cn]()

**ü§© For Software Engineering of Web3**

## Feature

**Web3, AI and Software Engineering**

- üîó Data crawler
- ü§î Data collecting, labelling and analyzing
- üëÄ Data visualization
- ü§ù Interact with Web3 blockchain networks
- üöÄ Build, train and evaluate AI models

### REPO1: SmartBERT

* An context embedding tool for smart contracts.
* Use BERT/Roberta model framework.
* Trained on 40,000+ real smart contracts.
* Serving APIs: [tokenize](http://api.smart.cas-ll.cn/smartbert/tokenize), [embed](http://api.smart.cas-ll.cn/smartbert/embed), [tree](http://api.smart.cas-ll.cn/smartbert/tree).

### REPO2: Smart Intent Nerual Network

- ü§© SmartIntentNN is an AI tool powered by [Tensorflow.js](https://github.com/tensorflow/tfjs), to detect malicious intents in smart contracts.
- ‚öô Ô∏è Web pages are generated by gitlab CI/CD, referring to [frontend repo](https://gitlab.com/web3se/smartvue).
- üì± Pages: [Home](https://www.web3se.top/), [Highlight](https://www.web3se.top/highlight/), [Evaluation](https://www.web3se.top/evaluate/), are laid out to test model online.
- üïµ Anyone can upload smart contract code for testing, click "Upload my smart contract"‚Äç
- üîò Click ![example-predict-button](./imgs/example-btn.png) to detect the malicious intents in smart contracts.

## Dataset

__üíΩ Use our [dataset](http://api.smart.cas-ll.cn)__

The above URL is a `GET/POST JSON` API.
You can query data by changing the parameter `key` in the URL.

### Dataset of Intent

<http://api.smart.cas-ll.cn/data/intent?key=1>

### Dataset of Vulnerability

<http://api.smart.cas-ll.cn/data/vulnerability>

| Vulnerability Type           | Num  |
| ---------------------------- | ---- |
| block number dependency (BN) | 405  |
| dangerous delegatecall (DE)  | 96   |
| ether frozen (EF)            | 96   |
| ether strict equality (SE)   | 365  |
| integer overflow (OF)        | 589  |
| reentrancy (RE)              | 1217 |
| timestamp dependency (TP)    | 311  |
| unchecked external call (UC) | 1198 |

### Models

__ü§ñ Try our [models](https://gitlab.com/web3se/smartintent/-/releases)__

__Use our models__, download the models and copy/move them to `/tf/models/`.

- For intent highlight __K-means model__, move it to the root path of `/tf/models/kmeans-model.json`.
- For __mymodel-\*.zip__, unzip them and then move to the directories of corresponding filename, e.g., `mymodel-bilstm.zip` corresponds to `/tf/models/mymodel_bilstm/`.
- For __universal sentence encoder__, download from: [https://tfhub.dev/google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4). ‚¨áÔ∏è
- For directly getting embedding vectors, request [code/embedding](https://api.smart.cas-ll.com/code/embedding?key=1). ‚¨áÔ∏è (param `key` is __PK__ in database)

## Install

Before using the program, you need to install __nodejs__ and __npm__ tools first, then you install dependencies.

Our recommended version is Node.js v16+.

```bash
yarn
# or
npm install
```

## Prepare

Prepare a csv dataset of smart contracts and put them in the directory `/db`.

For BSC Mainnet, download the latest verified contracts from [BSC verified contracts addresses](https://bscscan.com/exportData?type=open-source-contract-codes).

Then, you need to config your own _bscscan_, _etherscan_ [API](https://docs.bscscan.com/api-endpoints/contracts) secret keys in `/src/config/network.json`.

## Database

If you would like to set up a localhost database, we prepare a `docker-compose.yml` for you.

To start a MySQL docker service locally, try:

```bash
yarn mysql
```

To connect to local mysql database, you can create and modify the `.env` as the following content:

```
# DB
DB_DIALECT=mysql
MYSQL_HOST=localhost
MYSQL_USER=web3
MYSQL_PASS=web3
MYSQL_DB=web3

# SMARTBERT
EMBED_API=http://192.168.99.35:8000
WEB_PORT=8081

# UNIAI
UNIAI_API=http://192.168.41.52:3300
UNIAI_TOKEN=ReadingZhiDuJUN2023!
MODEL=GPT
SUBMODEL=gpt-3.5-turbo

TFJS=@tensorflow/tfjs-node

```

Then you can test your database connection and create the initial tables:

```bash

# test connect to databases
yarn DB test

# init tables, if table is not input, default is all the tables
yarn DB init [Table]

# drop tables, be careful!
yarn DB drop [Table]

```

## Web

**Serve Web APIs**

```bash
# development mode
yarn dev
# product mode
yarn start
# stop web service
yarn stop
```

**APIs for dataset**

- [data/get](http://api.smart.cas-ll.cn/data/get)
- [data/intent](http://api.smart.cas-ll.cn/data/intent)
- [data/vulnerability](http://api.smart.cas-ll.cn/data/vulnerability)

**SmartBERT APIs**

- [tokenize](http://api.smart.cas-ll.cn/smartbert/tokenize)
- [embed](http://api.smart.cas-ll.cn/smartbert/embed)
- [token/get](http://api.smart.cas-ll.cn/smartbert/tree)

**POST params**
1. `code` smart contract code content
2. `type` "solidity" or "vyper", the default is "solidity"

EXAMPLE: [embed](http://api.smart.cas-ll.cn/smartbert/embed) is used to convert smart contract code to embedding vectors:

```json
{
    text: 'input solidity code here',
    type: 'solidity'
}
```

## Operation

**Update Contract Table**

Contract API default is from BSC MainNet bscscan.com, you may change network in crawler/updateContract.js

```bash
# crawl from an address
yarn updateContract 0x0E09FaBB73Bd3Ade0a17ECC321fD13a19e81cE82

# crawl from csv file in /db
yarn updateContract csv [from=ContractAddress]

# crawl from token list on BSC
yarn updateContract tokens

# label out token contracts, BEP20, 721...
# also add creator, txhash info to Contract table
yarn updateContract labelToken [startId] [endId]

# remove contracts without SourceCode
yarn updateContract removeNull
```

**Update token info in Token table**

For token's basic info, price, bnb LP, busd LP, marketcap, transfers...

```bash
# need UI, from bscscan
yarn updateToken info [start]

# is honeypot and honeypot info
yarn updateToken honeypot [start]

# remove null token
yarn updateToken removeNull

# update and label token risks
# need UI, from scamsniper
yarn updateToken risk [start]
```

**Update from safu**

```bash
# update scam possibility code
yarn safu scam [start] [end]

# simulate buy and update honeypot
yarn safu honeypot [start] [end]
```

**Select data from tables**

```bash
# count total num of contracts
yarn contract count

# get contract info by id, address
yarn contract get [id|address]

# get contract max id
yarn contract max

# check contract address exsited
yarn contract check [address]

# get contracts without SourceCode
yarn contract findNull

# remove contracts without SourceCode
yarn contract removeNull

# get contract network by address
yarn contract network [address]

# get token by id
yarn token get [id]

# get token by address
yarn token get [address]

# count token table rows
yarn token count

# get code data from codes by Id, Address
yarn code get [Id|Address]

# get max Id of code table
yarn code max

# get max compiled code Id from code table
yarn code max-compile

# get max embedding code Id from code table
yarn code max-embedding

# get word id
yarn word getId [word piece]

# get word by id
yarn word getWord [word id]

# count word pieces in table
yarn word count

# get code and risk relations
yarn data code-risk [Id|Address]

# count total types of contracts
yarn data count-token-type

# count different scam intents of risk
yarn data count-scam

# count different levels of risk
yarn data count-risk

# count total honeypots amount in database
yarn data count-honeypot

# generate a json or txt dataset file
yarn data code-txt
yarn data json-txt
```

**Compile and clean source code**

```bash
# compile all to low-level code
yarn updateCode all [start] [end]

# remove unconerned code: comments, heads, imports...
yarn updateCode clean [start] [end]

# extract functions tree map from contracts
yarn updateCode tree [start] [end]
```

**Tokenizer and embedding**

In this project, we use [sentence-piece](https://github.com/google/sentencepiece) as the tokenizer. It implements __BPE__ and __unigram language model__.

If you would like to generate your own dataset tokenizer model, you need to prepare a `data.txt` file in `/db/data.txt`.

Install python3 and pip.

```bash
# generate the data.txt
yarn data code-txt
yarn data json-txt

# go to sentence-piece dir
cd tensorflow/models/sentence-piece

# run python
pip install sentencepiece

python3 ./spm.py
```

Then you will get two tokenizer files: `sentencepiece.model` and `sentencepiece.vocab`

We have wrapped a __JS version__ of sentence-piece for this project.

To test sentence-piece tokenizer:

```bash
# get a contract's word piece tokens listed by functions
yarn tokenizer get [Id|Address]

# get the max word piece tokens of all the contracts
yarn tokenizer max [start]

# update word dictionary
yarn tokenizer update-word [start]

# count functions in a contract
yarn tokenizer count-fun [id]

# count a max functions contract in table
yarn tokenizer max-fun [start]
```

## Tensorflow.js

__K-means Intent Highlight Model__

```bash
# train k-means model
yarn highlight train [fromId] [slice] [rate] [maxIter]

# load trained highlight k-means model
yarn highlight load

# predict by highlight k-means cluster
yarn highlight predict [Id]
```

__MyModels__

```bash
# train mymodel using BiLSTM and intent highlight scale
yarn mymodel-bilstm-high-scale train [batches] [batchSize] [epoch] [fromId]

# evaluate mymodel using BiLSTM and intent highlight scale
yarn mymodel-bilstm-high-scale evaluate [fromId] [slice] [batchSize]

# predict mymodel using BiLSTM and intent highlight scale
yarn mymodel-bilstm-high-scale predict [fromId] [slice]

# model summary
yarn mymodel-bilstm-high-scale summary

# train model using LSTM 
yarn mymodel-lstm evaluate [fromId] [slice] [batchSize]
yarn mymodel-lstm evaluate [batches] [batchSize] [epoch] [fromId]
yarn mymodel-lstm predict [fromId] [slice]

yarn mymodel-bilstm evaluate [fromId] [slice] [batchSize]
yarn mymodel-bilstm evaluate [batches] [batchSize] [epoch] [fromId]
yarn mymodel-bilstm predict [fromId] [slice]

yarn mymodel-cnn evaluate [fromId] [slice] [batchSize]
yarn mymodel-cnn evaluate [batches] [batchSize] [epoch] [fromId]
yarn mymodel-cnn predict [fromId] [slice]

yarn mymodel-bilstm-high-asc evaluate [fromId] [slice] [batchSize]
yarn mymodel-bilstm-high-asc evaluate [batches] [batchSize] [epoch] [fromId]
yarn mymodel-bilstm-high-asc predict [fromId] [slice]

yarn mymodel-bilstm-high-desc evaluate [fromId] [slice] [batchSize]
yarn mymodel-bilstm-high-desc evaluate [batches] [batchSize] [epoch] [fromId]
yarn mymodel-bilstm-high-desc predict [fromId] [slice]
```

We use [sequelize](https://sequelize.org/) to manage a database.

For the details of data structures, please refer to `crawler/Model.js`

## Paper

@article{huang2022deep,
  title={Deep Smart Contract Intent Detection},
  author={Huang, Youwei and Zhang, Tao and Fang, Sen and Tan, Youshuai},
  journal={arXiv preprint arXiv:2211.10724},
  year={2022}
}

@article{huang2022smartintentnn,
  title={SmartIntentNN: Towards Smart Contract Intent Detection},
  author={Huang, Youwei and Zhang, Tao and Fang, Sen and Tan, Youshuai},
  journal={arXiv preprint arXiv:2211.13670},
  year={2022}
}

## Resource

### DataSource

1. [https://bscscan.com/](https://bscscan.com/)
2. [https://etherscan.io/](https://etherscan.io/)
3. [https://tokensniffer.com/](https://tokensniffer.com/)
4. [https://bscheck.eu/](https://bscheck.eu/)
5. [https://scamsniper.net/](https://scamsniper.net/)
6. [https://aphd.github.io/smart-corpus/](https://aphd.github.io/smart-corpus/)
7. [https://dashboard.tenderly.co/explorer/](https://dashboard.tenderly.co/explorer/)
8. [https://tools.staysafu.org/](https://tools.staysafu.org/)

### Dependency

1. [TensorFlow.js](https://js.tensorflow.org/api/latest/)
2. [TensorFlow Hub](https://tfhub.dev/)
3. [Universal Sentence Encoder V4](https://tfhub.dev/google/universal-sentence-encoder/4)
